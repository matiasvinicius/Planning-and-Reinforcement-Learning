 2735/100000: episode: 1, duration: 22.588s, episode steps: 2735, steps per second: 121, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.982 [0.000, 2.000],  loss: --, mean_q: --, mean_eps: --
 5457/100000: episode: 2, duration: 21.937s, episode steps: 2722, steps per second: 124, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.989 [0.000, 2.000],  loss: --, mean_q: --, mean_eps: --
 8200/100000: episode: 3, duration: 22.505s, episode steps: 2743, steps per second: 122, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.978 [0.000, 2.000],  loss: --, mean_q: --, mean_eps: --
 10943/100000: episode: 4, duration: 20.995s, episode steps: 2743, steps per second: 131, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.987 [0.000, 2.000],  loss: --, mean_q: --, mean_eps: --
 13680/100000: episode: 5, duration: 20.336s, episode steps: 2737, steps per second: 135, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.031 [0.000, 2.000],  loss: --, mean_q: --, mean_eps: --
 16437/100000: episode: 6, duration: 20.550s, episode steps: 2757, steps per second: 134, episode reward:  3.000, mean reward:  0.001 [ 0.000,  1.000], mean action: 0.981 [0.000, 2.000],  loss: --, mean_q: --, mean_eps: --
 19157/100000: episode: 7, duration: 20.581s, episode steps: 2720, steps per second: 132, episode reward:  1.000, mean reward:  0.000 [ 0.000,  1.000], mean action: 0.992 [0.000, 2.000],  loss: --, mean_q: --, mean_eps: --
 21882/100000: episode: 8, duration: 20.441s, episode steps: 2725, steps per second: 133, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.991 [0.000, 2.000],  loss: --, mean_q: --, mean_eps: --
 24613/100000: episode: 9, duration: 20.278s, episode steps: 2731, steps per second: 135, episode reward:  2.000, mean reward:  0.001 [ 0.000,  1.000], mean action: 1.001 [0.000, 2.000],  loss: --, mean_q: --, mean_eps: --
 27345/100000: episode: 10, duration: 20.228s, episode steps: 2732, steps per second: 135, episode reward:  3.000, mean reward:  0.001 [ 0.000,  1.000], mean action: 0.980 [0.000, 2.000],  loss: --, mean_q: --, mean_eps: --
 30062/100000: episode: 11, duration: 20.243s, episode steps: 2717, steps per second: 134, episode reward:  3.000, mean reward:  0.001 [ 0.000,  1.000], mean action: 1.013 [0.000, 2.000],  loss: --, mean_q: --, mean_eps: --
 32809/100000: episode: 12, duration: 20.540s, episode steps: 2747, steps per second: 134, episode reward:  3.000, mean reward:  0.001 [ 0.000,  1.000], mean action: 1.002 [0.000, 2.000],  loss: --, mean_q: --, mean_eps: --
 35538/100000: episode: 13, duration: 19.531s, episode steps: 2729, steps per second: 140, episode reward:  5.000, mean reward:  0.002 [ 0.000,  1.000], mean action: 0.989 [0.000, 2.000],  loss: --, mean_q: --, mean_eps: --
 38274/100000: episode: 14, duration: 19.439s, episode steps: 2736, steps per second: 141, episode reward:  6.000, mean reward:  0.002 [ 0.000,  1.000], mean action: 1.007 [0.000, 2.000],  loss: --, mean_q: --, mean_eps: --
 40999/100000: episode: 15, duration: 19.305s, episode steps: 2725, steps per second: 141, episode reward:  6.000, mean reward:  0.002 [ 0.000,  1.000], mean action: 0.996 [0.000, 2.000],  loss: --, mean_q: --, mean_eps: --
 43723/100000: episode: 16, duration: 19.291s, episode steps: 2724, steps per second: 141, episode reward:  6.000, mean reward:  0.002 [ 0.000,  1.000], mean action: 1.000 [0.000, 2.000],  loss: --, mean_q: --, mean_eps: --
 46446/100000: episode: 17, duration: 19.850s, episode steps: 2723, steps per second: 137, episode reward:  7.000, mean reward:  0.003 [ 0.000,  1.000], mean action: 1.007 [0.000, 2.000],  loss: --, mean_q: --, mean_eps: --
 49191/100000: episode: 18, duration: 21.089s, episode steps: 2745, steps per second: 130, episode reward:  7.000, mean reward:  0.003 [ 0.000,  1.000], mean action: 0.998 [0.000, 2.000],  loss: --, mean_q: --, mean_eps: --
 51927/100000: episode: 19, duration: 20.600s, episode steps: 2736, steps per second: 133, episode reward: 10.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.000 [0.000, 2.000],  loss: --, mean_q: --, mean_eps: --
 54671/100000: episode: 20, duration: 21.257s, episode steps: 2744, steps per second: 129, episode reward:  8.000, mean reward:  0.003 [ 0.000,  1.000], mean action: 1.002 [0.000, 2.000],  loss: --, mean_q: --, mean_eps: --
 57403/100000: episode: 21, duration: 20.676s, episode steps: 2732, steps per second: 132, episode reward: 11.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.020 [0.000, 2.000],  loss: --, mean_q: --, mean_eps: --
 60124/100000: episode: 22, duration: 19.941s, episode steps: 2721, steps per second: 136, episode reward: 12.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 0.988 [0.000, 2.000],  loss: --, mean_q: --, mean_eps: --
 62853/100000: episode: 23, duration: 18.723s, episode steps: 2729, steps per second: 146, episode reward: 12.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 0.991 [0.000, 2.000],  loss: --, mean_q: --, mean_eps: --
 65561/100000: episode: 24, duration: 18.410s, episode steps: 2708, steps per second: 147, episode reward: 11.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.013 [0.000, 2.000],  loss: --, mean_q: --, mean_eps: --
 68285/100000: episode: 25, duration: 19.068s, episode steps: 2724, steps per second: 143, episode reward: 12.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 0.989 [0.000, 2.000],  loss: --, mean_q: --, mean_eps: --
 71010/100000: episode: 26, duration: 18.608s, episode steps: 2725, steps per second: 146, episode reward: 15.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 0.993 [0.000, 2.000],  loss: --, mean_q: --, mean_eps: --
 73745/100000: episode: 27, duration: 19.421s, episode steps: 2735, steps per second: 141, episode reward: 15.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.006 [0.000, 2.000],  loss: --, mean_q: --, mean_eps: --
 76473/100000: episode: 28, duration: 20.183s, episode steps: 2728, steps per second: 135, episode reward: 16.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 0.990 [0.000, 2.000],  loss: --, mean_q: --, mean_eps: --
 79177/100000: episode: 29, duration: 18.396s, episode steps: 2704, steps per second: 147, episode reward: 17.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.006 [0.000, 2.000],  loss: --, mean_q: --, mean_eps: --
 81925/100000: episode: 30, duration: 18.842s, episode steps: 2748, steps per second: 146, episode reward: 19.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.003 [0.000, 2.000],  loss: --, mean_q: --, mean_eps: --
 84648/100000: episode: 31, duration: 19.889s, episode steps: 2723, steps per second: 137, episode reward: 17.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 0.996 [0.000, 2.000],  loss: --, mean_q: --, mean_eps: --
 87393/100000: episode: 32, duration: 18.561s, episode steps: 2745, steps per second: 148, episode reward: 21.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.000 [0.000, 2.000],  loss: --, mean_q: --, mean_eps: --
 90128/100000: episode: 33, duration: 20.569s, episode steps: 2735, steps per second: 133, episode reward: 18.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.011 [0.000, 2.000],  loss: --, mean_q: --, mean_eps: --
 92863/100000: episode: 34, duration: 20.526s, episode steps: 2735, steps per second: 133, episode reward: 21.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 0.992 [0.000, 2.000],  loss: --, mean_q: --, mean_eps: --
 95599/100000: episode: 35, duration: 20.554s, episode steps: 2736, steps per second: 133, episode reward: 18.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 0.993 [0.000, 2.000],  loss: --, mean_q: --, mean_eps: --
 98337/100000: episode: 36, duration: 21.588s, episode steps: 2738, steps per second: 127, episode reward: 19.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 0.989 [0.000, 2.000],  loss: --, mean_q: --, mean_eps: --
done, took 737.803 seconds
