{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6277b54f-7f79-4950-ab2a-3ca1d9e5d16f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import with_statement\n",
    "import matplotlib.pyplot as plt\n",
    "import gym\n",
    "import random\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, Convolution2D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from rl.agents import DQNAgent\n",
    "from rl.memory import SequentialMemory\n",
    "from rl.policy import LinearAnnealedPolicy, EpsGreedyQPolicy\n",
    "import numba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e110462a-4f36-4713-84e4-90e7dffa34db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(height, width, channels, actions):\n",
    "    model = Sequential()\n",
    "    model.add(Convolution2D(32, (8, 8), strides=(4, 4),\n",
    "              activation='relu', input_shape=(3, height, width, channels)))\n",
    "    model.add(Convolution2D(64, (4, 4), strides=(2, 2), activation='relu'))\n",
    "    model.add(Convolution2D(64, (3, 3), activation='relu'))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(512, activation='relu'))\n",
    "    model.add(Dense(256, activation='relu'))\n",
    "    model.add(Dense(actions, activation='linear'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_agent(model, actions):\n",
    "    policy = LinearAnnealedPolicy(EpsGreedyQPolicy(\n",
    "    ), attr='eps', value_max=1, value_min=.1, value_test=.2, nb_steps=100000)\n",
    "    memory = SequentialMemory(limit=1000, window_length=3)\n",
    "    dqn = DQNAgent(model=model, memory=memory, policy=policy,\n",
    "                   enable_dueling_network=True, dueling_type='avg',\n",
    "                   nb_actions=actions, nb_steps_warmup = 100000)\n",
    "    return dqn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"Freeway-v0\")\n",
    "height, width, channels = env.observation_space.shape\n",
    "actions = env.action_space.n\n",
    "model = build_model(height, width, channels, actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training for 100000 steps ...\n",
      "/home/vinicius/.local/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py:2426: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n",
      "  2725/100000: episode: 1, duration: 118.381s, episode steps: 2725, steps per second:  23, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.016 [0.000, 2.000],  loss: --, mean_q: --, mean_eps: --\n",
      "  5470/100000: episode: 2, duration: 116.325s, episode steps: 2745, steps per second:  24, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.995 [0.000, 2.000],  loss: --, mean_q: --, mean_eps: --\n",
      "  8194/100000: episode: 3, duration: 114.674s, episode steps: 2724, steps per second:  24, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.008 [0.000, 2.000],  loss: --, mean_q: --, mean_eps: --\n",
      " 10923/100000: episode: 4, duration: 115.105s, episode steps: 2729, steps per second:  24, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.992 [0.000, 2.000],  loss: --, mean_q: --, mean_eps: --\n",
      " 13669/100000: episode: 5, duration: 115.822s, episode steps: 2746, steps per second:  24, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.011 [0.000, 2.000],  loss: --, mean_q: --, mean_eps: --\n",
      " 16398/100000: episode: 6, duration: 115.003s, episode steps: 2729, steps per second:  24, episode reward:  1.000, mean reward:  0.000 [ 0.000,  1.000], mean action: 0.998 [0.000, 2.000],  loss: --, mean_q: --, mean_eps: --\n",
      " 19146/100000: episode: 7, duration: 115.851s, episode steps: 2748, steps per second:  24, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: --, mean_q: --, mean_eps: --\n",
      " 21893/100000: episode: 8, duration: 116.087s, episode steps: 2747, steps per second:  24, episode reward:  2.000, mean reward:  0.001 [ 0.000,  1.000], mean action: 0.985 [0.000, 2.000],  loss: --, mean_q: --, mean_eps: --\n",
      " 24635/100000: episode: 9, duration: 115.531s, episode steps: 2742, steps per second:  24, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.995 [0.000, 2.000],  loss: --, mean_q: --, mean_eps: --\n",
      " 27358/100000: episode: 10, duration: 114.706s, episode steps: 2723, steps per second:  24, episode reward:  3.000, mean reward:  0.001 [ 0.000,  1.000], mean action: 1.025 [0.000, 2.000],  loss: --, mean_q: --, mean_eps: --\n",
      " 30087/100000: episode: 11, duration: 115.158s, episode steps: 2729, steps per second:  24, episode reward:  6.000, mean reward:  0.002 [ 0.000,  1.000], mean action: 1.005 [0.000, 2.000],  loss: --, mean_q: --, mean_eps: --\n",
      " 32827/100000: episode: 12, duration: 115.189s, episode steps: 2740, steps per second:  24, episode reward:  5.000, mean reward:  0.002 [ 0.000,  1.000], mean action: 0.996 [0.000, 2.000],  loss: --, mean_q: --, mean_eps: --\n",
      " 35540/100000: episode: 13, duration: 114.100s, episode steps: 2713, steps per second:  24, episode reward:  6.000, mean reward:  0.002 [ 0.000,  1.000], mean action: 1.008 [0.000, 2.000],  loss: --, mean_q: --, mean_eps: --\n",
      " 38288/100000: episode: 14, duration: 115.506s, episode steps: 2748, steps per second:  24, episode reward:  4.000, mean reward:  0.001 [ 0.000,  1.000], mean action: 0.995 [0.000, 2.000],  loss: --, mean_q: --, mean_eps: --\n",
      " 41018/100000: episode: 15, duration: 116.311s, episode steps: 2730, steps per second:  23, episode reward:  5.000, mean reward:  0.002 [ 0.000,  1.000], mean action: 1.004 [0.000, 2.000],  loss: --, mean_q: --, mean_eps: --\n",
      " 43761/100000: episode: 16, duration: 115.158s, episode steps: 2743, steps per second:  24, episode reward:  5.000, mean reward:  0.002 [ 0.000,  1.000], mean action: 0.999 [0.000, 2.000],  loss: --, mean_q: --, mean_eps: --\n",
      " 46464/100000: episode: 17, duration: 113.883s, episode steps: 2703, steps per second:  24, episode reward: 10.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 0.986 [0.000, 2.000],  loss: --, mean_q: --, mean_eps: --\n",
      " 49190/100000: episode: 18, duration: 114.940s, episode steps: 2726, steps per second:  24, episode reward:  9.000, mean reward:  0.003 [ 0.000,  1.000], mean action: 0.992 [0.000, 2.000],  loss: --, mean_q: --, mean_eps: --\n",
      " 51911/100000: episode: 19, duration: 115.042s, episode steps: 2721, steps per second:  24, episode reward: 11.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 0.986 [0.000, 2.000],  loss: --, mean_q: --, mean_eps: --\n",
      " 54632/100000: episode: 20, duration: 114.507s, episode steps: 2721, steps per second:  24, episode reward: 10.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.024 [0.000, 2.000],  loss: --, mean_q: --, mean_eps: --\n",
      " 57345/100000: episode: 21, duration: 114.437s, episode steps: 2713, steps per second:  24, episode reward: 15.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 0.987 [0.000, 2.000],  loss: --, mean_q: --, mean_eps: --\n",
      " 60075/100000: episode: 22, duration: 114.850s, episode steps: 2730, steps per second:  24, episode reward: 14.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 0.993 [0.000, 2.000],  loss: --, mean_q: --, mean_eps: --\n",
      " 62784/100000: episode: 23, duration: 113.976s, episode steps: 2709, steps per second:  24, episode reward: 12.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.014 [0.000, 2.000],  loss: --, mean_q: --, mean_eps: --\n",
      " 65509/100000: episode: 24, duration: 114.524s, episode steps: 2725, steps per second:  24, episode reward: 13.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.008 [0.000, 2.000],  loss: --, mean_q: --, mean_eps: --\n",
      " 68243/100000: episode: 25, duration: 114.163s, episode steps: 2734, steps per second:  24, episode reward: 15.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.015 [0.000, 2.000],  loss: --, mean_q: --, mean_eps: --\n",
      " 70994/100000: episode: 26, duration: 114.531s, episode steps: 2751, steps per second:  24, episode reward: 15.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.002 [0.000, 2.000],  loss: --, mean_q: --, mean_eps: --\n",
      " 73717/100000: episode: 27, duration: 113.381s, episode steps: 2723, steps per second:  24, episode reward: 13.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 0.998 [0.000, 2.000],  loss: --, mean_q: --, mean_eps: --\n",
      " 76425/100000: episode: 28, duration: 113.047s, episode steps: 2708, steps per second:  24, episode reward: 17.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 0.997 [0.000, 2.000],  loss: --, mean_q: --, mean_eps: --\n",
      " 79143/100000: episode: 29, duration: 113.124s, episode steps: 2718, steps per second:  24, episode reward: 17.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 0.994 [0.000, 2.000],  loss: --, mean_q: --, mean_eps: --\n",
      " 81890/100000: episode: 30, duration: 114.928s, episode steps: 2747, steps per second:  24, episode reward: 16.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 0.991 [0.000, 2.000],  loss: --, mean_q: --, mean_eps: --\n",
      " 84604/100000: episode: 31, duration: 112.726s, episode steps: 2714, steps per second:  24, episode reward: 17.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.008 [0.000, 2.000],  loss: --, mean_q: --, mean_eps: --\n",
      " 87341/100000: episode: 32, duration: 113.914s, episode steps: 2737, steps per second:  24, episode reward: 19.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 0.994 [0.000, 2.000],  loss: --, mean_q: --, mean_eps: --\n",
      " 90079/100000: episode: 33, duration: 114.435s, episode steps: 2738, steps per second:  24, episode reward: 20.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.013 [0.000, 2.000],  loss: --, mean_q: --, mean_eps: --\n",
      " 92813/100000: episode: 34, duration: 113.677s, episode steps: 2734, steps per second:  24, episode reward: 20.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.004 [0.000, 2.000],  loss: --, mean_q: --, mean_eps: --\n",
      " 95538/100000: episode: 35, duration: 113.539s, episode steps: 2725, steps per second:  24, episode reward: 20.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.002 [0.000, 2.000],  loss: --, mean_q: --, mean_eps: --\n",
      " 98254/100000: episode: 36, duration: 113.198s, episode steps: 2716, steps per second:  24, episode reward: 20.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.000 [0.000, 2.000],  loss: --, mean_q: --, mean_eps: --\n",
      "done, took 4202.791 seconds\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 3, 51, 39, 32)     6176      \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 3, 24, 18, 64)     32832     \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 3, 22, 16, 64)     36928     \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 67584)             0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 512)               34603520  \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 3)                 771       \n",
      "=================================================================\n",
      "Total params: 34,811,555\n",
      "Trainable params: 34,811,555\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "dqn = build_agent(model, actions)\n",
    "dqn.compile(Adam(learning_rate=0.01))\n",
    "dqn.fit(env, nb_steps=100000, visualize=True, verbose=2)\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Testing for 10 episodes ...\n",
      "Episode 1: reward: 24.000, steps: 2727\n",
      "Episode 2: reward: 21.000, steps: 2743\n",
      "Episode 3: reward: 23.000, steps: 2739\n",
      "Episode 4: reward: 24.000, steps: 2739\n",
      "Episode 5: reward: 21.000, steps: 2739\n",
      "Episode 6: reward: 21.000, steps: 2722\n",
      "Episode 7: reward: 25.000, steps: 2737\n",
      "Episode 8: reward: 22.000, steps: 2737\n",
      "Episode 9: reward: 22.000, steps: 2727\n",
      "Episode 10: reward: 25.000, steps: 2739\n",
      "22.8\n"
     ]
    }
   ],
   "source": [
    "scores = dqn.test(env, nb_episodes = 10, visualize=True)\n",
    "print(np.mean(scores.history['episode_reward']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}